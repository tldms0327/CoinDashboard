# Data Stream Analysis with EMR

EMRì—ì„œ ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸° ìœ„í•´ ì‹œë„í–ˆë˜ ë°©ë²•ë“¤ì„ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤.

```
spec:
	spark v2.4.7
	zeppelin v0.9.0
	scala v2.1.1
	java v1.8.0
	python v3.7.9
```

### kinesisì™€ emrì„ ì§ì ‘ ì—°ê²°í•˜ê¸°(ì‹¤íŒ¨)

1. **spark**ì—ì„œ **kinesis stream**ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì¤€ë¹„ì‚¬í•­

    spark applicationì— í•„ìš”í•œ jar íŒŒì¼ì„ mavenì—ì„œ ë‹¤ìš´ë¡œë“œí•˜ê³ , dependencyë¥¼ ì„¤ì •í–ˆë‹¤.

    [](https://mvnrepository.com)

    ```bash
    #amazon-kinesis-client-1.8.0.jar
    #spark-sql-kinesis-asl_2.11-2.4.2-atlassian-1.jar
    #spark-streaming-kinesis-asl_2.11-2.4.7.jar

    $ sudo cp {jars} /usr/lib/spark/jars
    # or
    $ spark-submit --jars {jars}
    ```

    ì œí”Œë¦° ì¸í„°í”„ë¦¬í„°ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ì„¤ì •í•´ì¤€ë‹¤.

2. **zeppelin**ì—ì„œ **kinesis stream**ì— ì—°ê²°í•˜ê¸°

    ```python
    from pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream
    from pyspark.streaming import StreamingContext
    from pyspark import StorageLevel
    from pyspark import SparkContext

    spark_session = SparkSession.builder.getOrCreate()
    sc = spark_session.sparkContext
    k = StreamingContext(sc, 10)
    # <pyspark.streaming.context.StreamingContext object at 0x7fc76ef42510>

    appName = "Test_Consumer" # dynamo db í…Œì´ë¸” ì´ë¦„, ì—†ìœ¼ë©´ ìë™ìƒì„±ë¨!checkpointì €ì¥
    streamName = "coinBoard"
    endpoint_url = "https://kinesis.ap-northeast-2.amazonaws.com"
    regionName = "ap-northeast-2"

    kinesis = KinesisUtils.createStream(ssc, appName, streamName, endpoint_url, regionName, InitialPositionInStream.TRIM_HORIZON, 1)

    kinesis.pprint([0]) # -> ì•„ë¬´ê²ƒë„ ì•ˆë‚˜ì˜´ ã… ã… 
    ```

    ìœ„ì™€ ê°™ì´ ì‹¤í–‰í•˜ë©´ `StreamingContext`ê°ì²´ê°€ ìƒì„±ëœë‹¤. ë°›ì•„ì˜¨ ë°ì´í„°ì— ê°„ë‹¨í•œ ì—°ì‚°ì„ í•˜ì—¬ ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³ ì í–ˆë‹¤.

    ```python
    from pyspark.streaming.kinesis import KinesisUtils, InitialPositionInStream
    from pyspark.streaming import StreamingContext
    from pyspark import StorageLevel
    from pyspark import SparkContext

    appName = "test" # dynamo db í…Œì´ë¸” ì´ë¦„, ì—†ìœ¼ë©´ ìë™ìƒì„±ë¨!
    streamName = "coinBoard"
    endpoint_url = "[https://kinesis.ap-northeast-2.amazonaws.com](https://kinesis.ap-northeast-2.amazonaws.com/)"
    regionName = "ap-northeast-2"

    def process_stream(record, spark):
    	if not record.isEmpty():
    		df = spark.createDataFrame(record)
    		df.show()

    def main():
    	spark_session = SparkSession.builder.getOrCreate()
    	sc = spark_session.sparkContext
    	ssc = StreamingContext(sc, 5)
    	kinesis = KinesisUtils.createStream(ssc, appName, streamName, endpoint_url, regionName, InitialPositionInStream.TRIM_HORIZON, 1)
    	
    	kinesis.foreachRDD(lambda rdd: process_stream(rdd, spark))
    		ssc.start()
    	ssc.awaitTermination()

    main()
    ```

    ```python
    Py4JJavaError: An error occurred while calling o232.awaitTermination.
    : java.net.ConnectException: Connection refused (Connection refused)
    	at java.net.PlainSocketImpl.socketConnect(Native Method)
    	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)
    	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)
    	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)
    	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
    	at java.net.Socket.connect(Socket.java:607)
    	at java.net.Socket.connect(Socket.java:556)
    	at java.net.Socket.<init>(Socket.java:452)
    	at java.net.Socket.<init>(Socket.java:262)
    	at javax.net.DefaultSocketFactory.createSocket(SocketFactory.java:277)
    	at py4j.CallbackConnection.start(CallbackConnection.java:226)
    	at py4j.CallbackClient.getConnection(CallbackClient.java:238)
    	at py4j.CallbackClient.getConnectionLock(CallbackClient.java:250)
    	at py4j.CallbackClient.sendCommand(CallbackClient.java:377)
    	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
    	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
    	at com.sun.proxy.$Proxy62.call(Unknown Source)
    	at org.apache.spark.streaming.api.python.TransformFunction.callPythonTransformFunction(PythonDStream.scala:92)
    	at org.apache.spark.streaming.api.python.TransformFunction.apply(PythonDStream.scala:78)
    	at org.apache.spark.streaming.api.python.PythonTransformedDStream.compute(PythonDStream.scala:246)
    	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
    	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:342)
    	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
    	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
    	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:341)
    	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416)
    	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:336)
    	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:334)
    	at scala.Option.orElse(Option.scala:289)
    	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:331)
    	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
    	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:122)
    	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:121)
    	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
    	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
    	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
    	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:121)
    	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
    	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
    	at scala.util.Try$.apply(Try.scala:192)
    	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
    	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
    	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
    	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
    	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)

    (<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError('An error occurred while calling o232.awaitTermination.\n', JavaObject id=o241), <traceback object at 0x7f5036c11370>)
    ```

    ìœ„ì™€ ê°™ì´ Connection refused ë˜ë©° ë°ì´í„°ë¥¼ ë°›ì•„ì˜¤ì§€ ëª»í–ˆë‹¤... ë³´ì•ˆ ì„¤ì •ë„ ë³´ê³ , jar íŒŒì¼ ë²„ì „ë„ ë°”ê¿”ê°€ë©° ì—¬ëŸ¬ê°€ì§€ ì‹œë„ë¥¼ í•´ë´¤ì§€ë§Œ ì•ˆ ë¼ì„œ ë‹¤ë¥¸ ë°©ë²•ì„ ëª¨ìƒ‰í–ˆë‹¤. ì‹œê°„ì´ ì—†ì–´ì„œ ã… .ã… 

[Spark Streaming + Kinesis Integration](https://spark.apache.org/docs/2.4.7/streaming-kinesis-integration.html)

### kinesis â†’ s3 â†’ emrë¡œ ì—°ê²°í•˜ê¸°(ì„±ê³µ)

1. **kinesis firehose**, **Glue**ë¥¼ ì´ìš©í•˜ì—¬ producerì—ì„œ ë³´ë‚¸ ë°ì´í„°ì— schemaë¥¼ ì”Œì›Œ **parquet**ë¡œ **S3**ì— ì „ì†¡í–ˆë‹¤.
2. emrì—ì„œ S3ë¥¼ **ìŠ¤íŠ¸ë¦¼ í˜•ì‹**ìœ¼ë¡œ ì½ëŠ”ë‹¤.

```python
from pyspark.sql.functions import *
from pyspark.sql import Row
from pyspark.sql.types import *

df_schema=spark.read.parquet("s3://coinboard2/*.parquet")
user_schema = df_schema.schema
parquet_sdf = spark.readStream.schema(user_schema).parquet("s3://coinboard2/*")
# count = parquet_sdf.groupBy('symbol').count()
# count.writeStream.outputMode("complete").format("console").start()

parquet_sdf.writeStream.queryName("test").format("memory").start()
```

ì²˜ìŒ ì‹œë„í–ˆì„ë•Œ, zeppelin userê°€ admin groupì— ì¶”ê°€ë˜ì§€ ì•Šì•„ /mnt/tmp í´ë”ì— ëŒ€í•œ ê¶Œí•œì´ ì—†ì–´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ë‹¤. spark-shellì—ì„œ ì‹¤í–‰í–ˆì„ë•Œ ë¬¸ì œì—†ì´ ì‘ë™ë˜ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤. ìš°ë¦¬ëŠ” ì œí”Œë¦°ì—ì„œ ì§€í‘œë¥¼ ë§Œë“¤ê³  ë””ë²„ê¹…í•´ì•¼ í•˜ë¯€ë¡œ, zeppelin ìœ ì €ë¥¼ hadoopadmin ê·¸ë£¹ì— ì¶”ê°€í–ˆë‹¤. 

```bash
$ group zeppelin # zeppelin : zeppelin
$ sudo gpasswd -a zeppelin hdfsadmingroup
$ group zeppelin # zeppelin : zeppelin hdfsadmingroup
```

`writeStream` í•  ë•Œ `queryName`ì„ ì„¤ì •í•˜ë©´ í…Œì´ë¸”ë¡œì„œ ì ‘ê·¼ ê°€ëŠ¥í•˜ë‹¤.

![Data%20Stream%20Analysis%20with%20EMR%20ec4d7544f5e8495ea49c4792ed75ff83/_2021-06-24__2.37.53.png](Data%20Stream%20Analysis%20with%20EMR%20ec4d7544f5e8495ea49c4792ed75ff83/_2021-06-24__2.37.53.png)

**ì„±ê³µ..!** ğŸ¤©